{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys, os\n",
    "libs_path = (Path(os.path.abspath(os.path.join('..'))).parent)\n",
    "sys.path.append(str(libs_path))\n",
    "\n",
    "from libs.corpus import get_corpus\n",
    "import re\n",
    "import requests\n",
    "from http.client import RemoteDisconnected\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from libs.storage import get_crawled_dataset, save_crawled_dataset\n",
    "from urllib.parse import urlparse\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_dataset(dataset_name: str):\n",
    "    corpus = dict(list(get_corpus(dataset_name).items())[:500])\n",
    "\n",
    "    new_docs = []\n",
    "    for doc in corpus.values():\n",
    "        new_docs.extend(_expand_document_with_crawled_data(doc))\n",
    "\n",
    "    for doc in new_docs:\n",
    "        corpus[str(uuid.uuid4())] = doc\n",
    "\n",
    "    save_crawled_dataset(corpus, dataset_name)\n",
    "\n",
    "\n",
    "def __extractURLs(content):\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', content)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def _expand_document_with_crawled_data(doc_content: str) -> list:\n",
    "    document_included_urls = __extractURLs(doc_content)\n",
    "    new_docs = []\n",
    "    if len(document_included_urls) > 0:\n",
    "        for url in document_included_urls:\n",
    "            crawled_text = __crawl(url)\n",
    "            new_docs += crawled_text\n",
    "    return new_docs\n",
    "\n",
    "\n",
    "def __is_text_url(url):\n",
    "    # send a HEAD request to the URL to retrieve the headers\n",
    "    response = requests.head(url)\n",
    "\n",
    "    # check the Content-Type and Content-Disposition headers\n",
    "    content_type = response.headers['Content-Type']\n",
    "    content_disposition = response.headers.get('Content-Disposition', '')\n",
    "    if 'text' in content_type and 'attachment' not in content_disposition:\n",
    "        # check the file extension of the URL\n",
    "        resource = urlparse(url).path\n",
    "        file_extension = resource.split('.')[-1]\n",
    "        # array of common text files extensions\n",
    "        text_file_extensions = ['txt', 'html', 'htm', 'xml', 'csv', 'json', 'md', 'rst', 'php', 'asp', 'aspx', 'css',\n",
    "                                'js', 'py', 'rb', 'java', 'c', 'cpp', 'h', 'sh', 'bat', 'log', 'ini', 'conf', 'yml',\n",
    "                                'yaml']\n",
    "        if file_extension in text_file_extensions:\n",
    "            return True\n",
    "\n",
    "        # download a small portion of the response and check its contents\n",
    "        response = requests.get(url, stream=True)\n",
    "        content = response.raw.read(1024)\n",
    "        if all(32 <= c < 127 or c in (9, 10, 13) for c in content):\n",
    "            return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __crawl(url):\n",
    "    try:\n",
    "        if __is_text_url(url):\n",
    "            print(f\"crawling {url}\")\n",
    "            html = urlopen(url).read()\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "\n",
    "            # kill all script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()  # rip it out\n",
    "\n",
    "            # get text\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # ###### some text processing #######\n",
    "            # break into lines and remove leading and trailing space on each\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            # break multi-headlines into a line each\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            # drop blank lines\n",
    "            text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "            return text\n",
    "        else:\n",
    "            return ''\n",
    "    except (HTTPError, URLError, RemoteDisconnected) as e:\n",
    "        return ''\n",
    "    except Exception as e:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crawl_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m crawl_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlifestyle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m corpus \u001b[38;5;241m=\u001b[39m get_corpus(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlifestyle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corpus))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crawl_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "crawl_dataset(\"lifestyle\")\n",
    "\n",
    "corpus = get_corpus(\"lifestyle\")\n",
    "print(len(corpus))\n",
    "\n",
    "crawled_corpus = get_crawled_dataset(\"lifestyle\")\n",
    "print(len(crawled_corpus))\n",
    "\n",
    "\n",
    "# crawl_dataset(\"antique\")\n",
    "\n",
    "# corpus = get_corpus(\"antique\")\n",
    "# print(len(corpus))\n",
    "\n",
    "# crawled_corpus = get_crawled_dataset(\"antique\")\n",
    "# print(len(crawled_corpus))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
